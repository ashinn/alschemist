
(define-library (chibi math autodiff-test)
  (import (scheme base)
          (scheme list)
          (scheme write)
          (srfi 231)
          (chibi math autodiff)
          (chibi math linalg)
          (chibi math array-test-utils)
          (chibi test))
  (export run-tests)
  (begin
    (define (run-tests)
      (test-begin "(chibi math autodiff)")
      ;; scalar derivatives
      (let*-duals ((a 3.0)
                   (b (.+ a a)))
        (test '(2.0) (gradient b (list a)))
        (let ((label (dual-label b)))
          (test-assert (or (equal? label "+")
                           (equal? label (list "+" a a))))))
      (let*-duals ((a 3.0)
                   (b (.* 2.0 a)))
        (test '(2.0) (gradient b (list a))))
      (let*-duals ((a 2.0)
                   (b -3.0)
                   (c 10.0)
                   (e (.* a b))
                   (d (.+ e c))
                   (f -2.0)
                   (L (.* d f)))
        (test -8.0 (dual-value L))
        (test '(6. -4. -2. -2. -2. 4.) (gradient L (list a b c d e f))))
      (let*-duals ((x1 (const 2.0))
                   (x2 (const 0.0))
                   (w1 -3.0)
                   (w2 1.0)
                   (b (const 6.8813735870195432))
                   (x1w1 (.* x1 w1))
                   (x2w2 (.* x2 w2))
                   (x1w1+x2w2 (.+ x1w1 x2w2))
                   (n (.+ x1w1+x2w2 b))
                   (o (.tanh n))
                   (grads (gradient o (list w1 w2))))
        (test 1.0 (first grads))
        (test 0.0 (second grads)))
      ;; repeat the above test with arrays
      (let*-duals ((x (tensor '(2.0 0.0)))
                   (w (tensor '(-3.0 1.0)))
                   (b 6.8813735870195432)
                   (xw (.* x w))
                   (n (.+ (.sum xw) b))
                   (o (.tanh n)))
        (test-array (tensor '(1. 0.))
          (first (gradient o (list w)))))
      ;; more complex operations
      (let*-duals ((a (tensor '((0.1 0.2) (0.4 0.8))))
                   (b (.log a))
                   (c (.mean b))
                   (grads (gradient c (list a b))))
        (test -1.262864 (dual-value c))
        (test-array (tensor '((-2.302585 -1.6094379)
                              (-0.9162907 -0.22314355)))
          (dual-value b))
        (test-array (tensor '((2.5 1.25) (0.625 0.3125)))
            (first grads))
        (test-array (tensor '((0.25 0.25) (0.25 0.25)))
            (second grads)))
      ;; matrix multiplication
      (let*-duals ((a (tensor '((0.1 0.2 0.3) (0.4 0.5 0.6))))
                   (b (tensor '((1.0 2.0) (3.0 4.0) (5.0 6.0))))
                   (c (.@ a b))
                   (loss (.mean c))
                   (grads (gradient loss (list a b c))))
        (test-array (tensor '((0.25 0.25) (0.25 0.25)))
          (third grads))
        (test-array (tensor '((0.125 0.125)
                              (0.175 0.175)
                              (0.225 0.225)))
          (second grads))
        (test-array (tensor '((0.75 1.75 2.75) (0.75 1.75 2.75)))
          (first grads)))
      ;; rectify
      (let*-duals ((b (tensor '(-0.3 0.4 0.3)))
                   (t (tensor '(1.3 -0.4 3.3)))
                   (y (.+ (.rectify t) b))
                   (loss (.mean (.square y)))
                   (grads (gradient loss (list b))))
        (test 4.7066665 (dual-value loss))
        (test-array (tensor '(1.0 0.4 3.6)) (dual-value y))
        (test-array (tensor '(#i2/3 #i4/15 2.4)) (first grads)))
      ;; relu (from nn)
      (let ()
        (define (linear t)
          (lambda (weights)
            (.+ (.@ (first weights) (.transpose t))
                (second weights))))
        (define (relu t)
          (lambda (weights)
            (.rectify ((linear t) weights))))
        (let*-duals ((x (tensor '((2. 1. 3.1) (3.7 4. 6.1))))
                     (b (tensor '((0.) (0.))))
                     (t (tensor '(1.3 .4 3.3)))
                     (y ((relu t) (list x b)))
                     (loss (.mean (.square y)))
                     (grads (gradient loss (list x b))))
          (test 439.70225 (dual-value loss))
          (test-array (tensor '((13.23) (26.54))) (dual-value y))
          (test-array (tensor '((17.199 5.292 43.659)
                                (34.502 10.616 87.582)))
            (first grads))
          ;; the gradient of the constant
          (test-array (tensor '((13.23) (26.54))) (second grads))))
      ;; broadcasting
      (let*-duals ((tmp1 (tensor '((1.0 2.0) (3.0 4.0))))
                   (tmp2 (tensor '((1.0) (1.0))))
                   (tmp3 (.@ tmp1 tmp2))
                   (tmp4 (./ tmp1 tmp3))
                   (loss (.mean tmp4))
                   (grads (gradient loss (list tmp1 tmp2 tmp3 tmp4))))
        (test-array (tensor '((0. 0.) (0. 0.)))
          (first grads))
        (test-array (tensor '((-0.190476) (-0.309524)))
          (second grads))
        (test-array (tensor '((-0.083333) (-0.035714)))
          (third grads))
        (test-array (tensor '((.25 .25) (.25 .25)))
          (fourth grads)))
      (let*-duals ((tmp1 (tensor '((1.0 2.0) (3.0 4.0))))
                   (tmp2 (.sum-axis tmp1 1))
                   (tmp3 (./ tmp1 tmp2))
                   (tmp4 (.log tmp3))
                   (loss (.mean tmp4))
                   (grads (gradient loss (list tmp1 tmp2 tmp3 tmp4))))
        (test-array (tensor '((0.0833333 -0.0416667)
                              (0.0119048 -0.0089286)))
          (first grads))
        (test-array (tensor '((-0.166667) (-0.071429)))
          (second grads)))
      (test-end))))

;; Local Variables:
;; eval: (put 'test-array 'scheme-indent-function 1)
;; eval: (put 'let*-duals 'scheme-indent-function 1)
;; End:
